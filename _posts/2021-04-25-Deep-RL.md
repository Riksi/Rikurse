---
layout: post
title:  "Diving into Deep RL"
date:   2021-04-25 05:01:20 +0000
categories: paper ai
---
<style>
.blue{
    color:cornflowerblue;
}
</style>

## Finite MDPs

Learning in circumstances where reward is delayed and depends not just on the actions taken 
by an agent but the state in which the agent find itself at different times.

![Diagram showing the process of taking an action leading to a reward and a new state]({{ site.baseurl}}/assets/Finite-MDP-diagram.png) 

Starting from some state $S_0$, this process gives rise to a sequence along the lines of 

$$S_t \rightarrow A_t \rightarrow R_t \rightarrow S_{t+1}$$

Finding itself in state $S_t$ the agent takes action $A_t$ which yields reward $R_t$ and brings it to a new state $S_{t+1}$ or leads the state to change to $S_{t+1}$.

Captured <span class="blue">[what?]</span> by a conditional distribution:

$$p\left(s', r \vert s, a\right) = P\left\{S_{t}=s', R_{t}=t \vert S_{t-1}=s, A_t=a\right\}$$

Why Finite Markov Decision Process?
- Finite because finite possibilities for each of state, action and reward. 
- Markov because $S_{t-1}$ and $A_{t-1}$ are all that are needed to know what $S_t$ and $R_t$ will be - summing over all the probabilities for $(S_t, R_t)$ for a given $(S_{t-1}, A_{t-1})$ pair is 1. This comes about because somehow or other $S_{t-1}$ is able to capture everything from the past that is needed for the future. <span class="blue">(Question for myself: where does $A_{t-1}$ come into this?)</span>
- Decision Process because agent is making a decision at each step as to what action to take(???)

Because of the finite number of values can think of the quantities involved in terms of tensors, where the $p\left(s', r \vert s, a\right)$ is element (s', r) of an $\lvert\mathcal{S}\rvert \times \lvert\mathcal{R}\rvert$ matrix. This matrix is turn is a slice of an $\lvert\mathcal{S}\rvert \times \lvert\mathcal{R}\rvert \times \lvert\mathcal{S}\rvert \times \lvert\mathcal{A}\rvert$ tensor so to get  $p\left(s', r \vert s, a\right)$  you select element $p[s', r, s, a]$.


<p style="text-align:center">
    <img alt="Diagram showing p(s', r|s,a) in matrix form"
    src="{{ site.baseurl}}/assets/p_srsa.png" style="height:50%; width:50%;">
</p>


<span class="blue">TODO: Concrete examples of agent, environment, state, action, reward.</span>

Value functions determine what is the expected reward of a state or of a state action pair. They are defined with respect to policies which are probability distributions over actions given a state.

Defined as $v_\pi(s)$ - state value function for policy $\pi$ which is the expected value of the return given that that state $S_t=s$ and we are following policy $\pi$. 

$q_\pi$ is similar but the expectation is condition on both the state $S_t=s$ as well as the action taken from the state $A_t=a$ and subsequently following $pi$.

If you had vectors for each state or a matrices for each state action pair and each time you encountered that state you updated the number of times you have seen that state and reward gained then eventually this will converge to the true value. 

Whether this makes sense depends on the number of states. 

(s,a) pairs indicate path dependence. It matters what action took you to the next state.

Say you have a vector of values for each state for every policy. If all the elements are of one vector are >= than the corresponding elements of another then the policy is said to be better than or as good as the other.

We can think of a matrix of values for each state for each policy. The optimal one is that for which each state's value is the max across the policy dimension for that state. 

Now consider $q_\pi(s, a)$ as a 3d tensor. This comes about by adding the expected value of the reward given state and action to the optimal value for that state (broadcast across actions). Then could think in terms of maximising across policies for each given state to get the optimal q. If you then maximise this across actions you get the optimal v. 

Once you have the optimal v then you can get the policy as follows:

- One or more actions $a$ will be those that lead to the best v
- Any policy that has non-zero probability for these actions only will be optimal

You can trade of storing state action pairs with storing just states for the optimal v and doing a one step look ahead to find the best policy. 

Not actually useful to solve Bellman equation in practice because it essentially involves an exhaustive search. 