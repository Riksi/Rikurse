---
layout: post
title:  "101 papers"
date:   2021-01-12 2:01:20 +0000
categories: rc ai
---
<script>
    window.onload = function(){

        let h2 = document.querySelectorAll('h2')
        var entries = 0;
        for(elem of h2){
            if(elem.id != 'progress'){
                var s = elem.id.split('-')[0];
                if (s.match(/[0-9]+/) !== null){
                    var n = parseInt(s);
                    if (n > entries){
                        entries = n;
                    }
                }
            }
        }

    document.getElementById('done').innerText = entries;
    let total = parseFloat(document.getElementById('total').innerText)
    let percent = (100. * entries) / (1. * total)
    console.log(percent)
    document.getElementById('myBar').style.width = percent + "%"
    if (percent > 33){
        document.getElementById('myBar').style.backgroundColor = 'blue'
    }

    if (percent > 67){
        document.getElementById('myBar').style.backgroundColor = 'purple'
    }

    if (percent == 100){
        document.getElementById('myBar').style.backgroundColor = 'gold'
    }

    }
</script>
<style>
    #progressOuter{
        width: 80%;
        float: left
    }
    #details{
        width: 18%;
        float: right;
    }
    #myProgress {
    width: 100%;
    background-color: silver;
    }

    #myBar {
        width: 0;
        height: 20px;
        background-color: green;
    }

</style>

## Progress 
<div>
    <div id='progressOuter'>
        <div id="myProgress">
            <div id="myBar"></div> 
        </div>
    </div>
    <div id='details'>
        <span id='done'></span>
        <span>/</span>
        <span id='total'>101</span>
    </div>
</div>
<br>

## 1. Hierarchical Multi-Scale Attention for Semantic Segmentation ([link](https://arxiv.org/abs/2005.10821v1))

Different image sizes work best for segmentation different types of objects in an image. Large regions benefit from small sizes enabling a model to get more context whilst fine details require high resolution. They propose a method to dynamiically combine the segmentations for different image scales in a cascaded hierarchical fashion with images decreasing in size. For each image scale a backbone segmentation network predicts a segmentation and an attention map. This attention is then "divided" between the segmentation for this image scale and the previous (larger) image scale and used to conbine segmentation outputs from the present and the previous layers to produce the final output for the present layer. This can be repeated an arbitrary number of times with the possibility of using different scales for training and inference (as they do). They achieve state of the art on Cityscapes for which they also use Cityscapes coarse annotations via auto-learning.


## 2. Sequence Level Semantics Aggregation for Video Object Detection ([link](https://arxiv.org/abs/1907.06390v2))
Their method for video object detection works as follows:
1/ Given a frame and a region proposal in the frame, randomly sample $N$ other frames (which could from before or after this frame) 
2/ Generate region proposals for these
3/ Get a similarity scores between the proposals in this frame and other frames
4/ For each proposal in this frame aggregate the proposals from other frames based on their similarity to this one (it is not clear how this is them combined with the proposal in the present frame)
5/ Make predictions as usual using the aggregated features (they use a Faster-RCNN type model)
This way you are implicitly dealing with graph connecting objects between different frames in the video. The key idea is the semantic neighbourhood to which this gives rise is a source of more diverse features compared to the temporal neighbourhood where there is likely to be high feature redundancy. At test time they sample a larger number of frames and from those temporally later as well as earlier (if I understand right and which although it seems to benefit performance will limit application in live settings). They get SoTA on ImageNet VID and EPIC KITCHENS. 

## 3. Taming Transformers for High-Resolution Image Synthesis ([link](https://arxiv.org/abs/1907.06390v2))
Images are represented as a grid learning embeddings that is of a smaller size compared to the input. These grid then be flattened into sequence that is of a feasible size to train a large auto-regressive transformer model which can then be used to generate much large images than standalone transformers can. The embeddings are learned using a generative model (VQGAN). This model consists of an encoder which predicts a feature map. The feature vector of each pixel in the feature map is then matched to the nearest learned embedding and the resulting feature map of embeddings is input to a decoder which generates a image output. Having obtained this "codebook" of embeddings, a transformer model can be trained in an autoregressive manner to predict the next (according to some ordering of grid elements) embedding index as a $K$-way softmax output for $K$ embeddings. These are used to look up the embeddings from the codebook and input to the decoder to generate an image. The model outperforms a SoTA fully convolutional approach (PixelSNAIL) on a number of datasets as well as generating high quality high resolution images.  


## 4. Reformer: The Efficient Transformer ([link](https://arxiv.org/abs/2001.04451))
This is another approach to enable transformers to handle long sequences. The bottleneck is the attention map whose size is of order (N^2) where N is the sequence length. In this paper they note that the attention maps depend on the $\text{softmax}\left(\frac{QK^T}{\sqrt(d_k)}\right)$ they will dominated by elements which correspond to key and query vectors which are the closest to each other whilst other elements will be close to zero. Based on this insight they use a hashing mechanism which with high probability will place vectors that are close to each other (in the sense of having a large cosine similarity) in the same bins. The attention weights then only need to be found for pairs of keys and queries in the same bin with pairs in different bins giving rise to a weight of zero. If the the bin sizes are much smaller than $N$ then this approach will be a lot cheaper. One change they make to the regular model is to define the key vectors as the unit vectors in the direction of the query vectors so that $k_z = \frac{q_z}{\left \lVert q_z \right \rVert}$, which ensures that the hashes of $k_z$ and $q_z$ are the same and alleviates the problem of very uneven numbers of keys and queries in a bucket. I presumed that this means that 1/ they don't do a key transform of the input in self-attention layers prior to applying attention; 2/ they don't use the encoder outputs / memory to generate the key vectors when obtaining the attention weights to apply to the encoder outputs. I am not sure if this is the case but it looks like all the attention heads are self-attention with the difference that an element does not attend to itself unless there is no other element available e.g. if it is the first element in a sequence. They also experiment with adding reversible layers as in the reversible ResNet. The resulting model has a performance comparable to the regular transformer on a number of datasets and problems (enwiki8, WMT, imagenet64). 

## 5. Adaptive Computation Time for Recurrent Neural Networks ([link] https://arxiv.org/abs/1603.08983))
The key idea here is that a model should be able dynamically decide how much computation time to spend on a task based on its complexity. Consider an addition problem with a sequence of numbers of variable lengths. Adding a larger number will be more involved than adding a smaller one and the model should be able to adapt accordingly. This can also occur with non-sequential inputs where the prediction is more difficult to make for some inputs compared to others. The model works by modifying a normal RNN cell so that it kind of becomes an RNN itself. The difference is that the sequence involved is the number of computation steps rather than the data itself. We distinguish between data timesteps $t$ and number of compute steps $i$. Each iteration receives the entire input at timestep $t$ with an indicator when $i=0$ to inform the cell that the computation has just started for this data element. The cell generates and output and state at each compute step $i$ and the state is fed back for the next compute step. (When $i=0$ the state from the previous timestep, or when $t=0$ the initial state, is used). The cell makes an additonal prediction at every step $i$ which is used to decide if it should stop computing for element $t$. These predictions are also used to generate weights for aggregating the outputs and states from each compute step $i$. The model is trained using regular losses for the problem and an additional loss that penalises long computation times. The model outperforms baseline models which don't use adaptive computation (i.e. where the RNN cell behaves like a regular RNN cell with just one compute step) on a number of algorithmic tasks (parity, logic, addition, sorting).


## 6. A unifying mutual information view of metric learning: cross-entropy vs. pairwise losses ([link](https://arxiv.org/abs/2003.08983v2))
They show that simply training a ResNet50 classifier with cross-entropy loss will yield embeddings that can be used for image retrieval resulting in high recall (beating SoTA for 3 out of 4 datasets for which they run experiments). Various kinds of metric learning losses can be split into a "contrastive" and a "tightness" part. The first part encourages embeddings for similar images to lie close together and the second encourages those from dissimilar images to lie far apart. The goal is to maximise the mutual information $\mathcal{I}(\hat{Z}; Y)$ between the features and the labels. This can be expressed as $\mathcal{I}(\hat{Z};Y) = \mathcal{H}(\hat{Z}) - \mathcal{H}(\hat{Z} \vert Y)$. This will be maximised if features are far apart but those for a given class are close together. They show that the contrastive and tightness parts of other losses have similar properties to $\mathcal{H}(\hat{Z})$ and $\mathcal{H}(\hat{Z} \vert Y)$ respectively. The key difference between CE loss and these losses is that the later doesn't incorporate any pairwise distances of the inputts. However they show that the CE loss can be represented as the sum of two components. The CE loss evaluated at the parameters $\theta^\*_1$ and $\theta^\*_2$ that minimise each component turns out to be a function of the pairwise distances of the inputs and this expression can be split into a contrastive and tightness part. This is defined as the Pairwise CE loss They argue that over the course of training  $\theta^\*_1$ and $\theta^\*_2$ become very close so that $\theta^\* \approx \theta^\*_1 \approx \theta^\*_2$ and thus the optimal CE loss at a given time-step is close to the Pairwise CE loss. Thus optimising the regular CE loss becomes an approximate upper-bound optimisation of the Pairwise CE loss.

## 7. pixelNeRF: Neural Radiance Fields from One or Few Images ([link](https://arxiv.org/abs/2012.02190v1))
This is an extension of the NeRF architecture which leverages image features to enable training with limited numbers of views and to train a model that can represent different scenes. Instead of just asking the model "what is the colour at point $p$ of this scene when viewed from direction $d$?", you now ask "what is the colour at point $p$ when viewed from direction $d$ of any scene relative to a view where point $p$ has features $z_p$?". The way it works is by obtaining features for a known view and interpolating these features at the query point and passing them into the model as inputs along with the position and direction. A difference from the original model is that they also input the query viewing direction (which is relative to the viewing direction of the input image) since it is useful for the model to know how different this is relative to the input view. It is also possible to input more than one view at test time by independently obtaining features for each view and aggregating them before making a final prediction.

## 8. Object-Centric Neural Scene Rendering ([link](https://arxiv.org/abs/2012.08503v1))
In the original NeRF model you just predict what is the colour along a direction $d$ to the camera origin. You only care about the outgoing rays from that point. You don't think about the incoming light which is reflected from that point and gives rise to the colour in the image. In this paper they predict the fraction incoming light from different light sources that hits a  point and use that to obtain the value of the outgoing ray. In contrast the original model simply predicts the final value of outgoing ray without considering how it comes about which prevents it from explicitly modelling interactions between objects. The train a separate model for each object and they compose the output using predictions from different models. Their model can also be used to predict shadows due to the some other object obstructing the light as well as secondary light rays that result from primary rays getting reflected from an object (and indeed the model can accomodate an arbitrary number of bounces).

## 9. Universal Transformers ([link](https://arxiv.org/abs/1807.03819))
The model in this paper is inspired by the [Adaptive Computation Time (ACT) RNN](https://arxiv.org/abs/1603.08983) but applies this approach to a Transformer. Similar to the regular Transformer model they use an encoder-decoder architecture where the encoder block comprises a self-attention block and a transition block (plus LayerNorm and Dropout) and the decoder block comprises self-attention, memory attention and a transition block. The key difference is that they have just a single encoder and decoder block. Encoding consists of applying the same encoder block, initially to the input and then to the output from the previous application for a number of timesteps. The output of this is passed to the decoder which is then repeatedly run for a number of timesteps. The number of timesteps can be determined beforehand or decided dynamically on a per-symbol basis similar to ACT-RNN using a halting mechanism as in ACT-RNN to decide whether to stop computing for a particular symbol. The difference is that since the whole sequence needs to be present each time to use in self-attention, the output for the symbols which have been halted is simplied copied over to the next stage. This model is particularly good at algorithmic tasks at which the regular transform struggles. They show that unlike the regular Transformer it is Turing-complete. It also outperforms regular transformers on language tasks and manages to get SoTA on the LAMBDADA language modelling task.

## 10. Bottleneck Transformers for Visual Recognition ([link](https://arxiv.org/abs/2101.11605))
This paper adopts a simple but effective approach of replacing lower resolution blocks of ResNet with Attention blocks and achieves instance segmentation performance which surpasses the previous best published results using a single model and single scale.

ResNet has residual blocks of the form `{Conv1x1(N1, N2)->Conv3x3(N2, N2)->Conv1x1(N2, N3)}` where `N1 > N2`, `N2 < N3` (except for the first conv `N2 = N1`). Compare this to the regular transformer which has the form `{Embedding(T, D1)}->[{Attention(D1, D1)}->{Dense(D1, D2)->Dense(D2, D1)}]+` where curly brackets `{..}` indicate residual blocks and `D2 > D1` and the number of tokens `T` may also be larger than `D1`. If you unroll it a bit you get `{Embedding(T, D1)}->{Attention(D1, D1)}->{Dense(D1, D2)->Dense(D2, D1)}-{Attention(D1, D1)}->{Dense(D1, D2)->Dense(D2, D1)}...`. Now if you move the skip connection so that it begins and ends between the `Dense` blocks (or before the `Embedding` block the first time) you get `{Embedding(T, D1)->{Attention(D1, D1)->Dense(D1, D2)}->{Dense(D2, D1)-Attention(D1, D1)->Dense(D1, D2)}->{Dense(D2, D1)->...}->...`, which makes the residual blocks into bottleneck blocks. The one difference when using this type of block in ResNet is that the dense layers are replaced with convolutional layers. 

They use relative positional encodings (which were found to be better than absolute ones). While models like ViT replace CNNs with Transformers, this is a hybrid backbone. Differently from DETR, this uses Transformer modules in the feature extraction step and uses regular detection heads whereas DETR uses a ResNet backbone followed by a Transformer based detection module. The resulting model named BoTNet also improves over various baseline architectures (SENets, Efficients, ResNets of different sizes) on ImageNet. 

## 11. Learning Deep Representations of Fine-grained Visual Descriptions ([link](https://arxiv.org/abs/1605.05395))
The task is to learn features from text descriptions of images of different classes that can be used for image retrieval and zero-shot classification. The way zero-shot classification works in this case is that you have text features from different classes and you pick the class whose average embedding yields the highest dot product with a given image. These will be new classes not seen at training time. The idea is that the model learns to project text containing information about a class to the same feature space as an image from that class. 

The data consists of the Caltech Birds dataset and Oxford Flowers dataset both of which have 100 or more classes and they obtain their own fine grained text  annotations 

Given an image embedding $\theta(v)$, find the class $y$ such such that the average dot product of the text embeddings from that class $\phi(t)$ with $\theta(v)$ is maximised. 

$$F(v, t) = \theta(v)^T\phi(t) \\
f_v(v) = \underset{y \in \mathcal{Y}}{\arg \max}\mathbb{E}_{t \sim \mathcal{T}(y)}\left[F(v, t)\right]$$

This is also reversed to classify the text embedding (although this is only done during training).

The image embeddings are fixed but the model is described as symmetrical in the sense that there is a pair of losses. 

$$\frac{1}{N} \sum_{n=1}^N l_v(v_n, t_n, y_n) + l_t(v_n, t_n, y_n)$$

One encourages all text embeddings from a given class to lie close to an image embedding from that class and the other encourages each text embedding from a class to lie close to all the images from that class. They experiment with various encoders to obtain the text features and find that a Word-CNN-RNN trained on fine-grained annotations and the symmetrical loss is successful in most cases but for Caltech Birds, just using one-hot encoded attributes as the input does better for retrieval. 

The loss $l_v$ is as follows and $l_t$ is similar with $v$ taking th place of $t$, where is $\Delta(y_n ,y)$ is the 0-1 loss:

$$l_v(v_n, t_n, y_n) = \max(0, \delta(y_n ,y) + \mathbb{E}_{t \sim \mathcal{T}(y)} \left[F(v_n, t) - F(v_n, t_n)\right])$$

This is the average difference between $F(v_n, t), t \sim T(y)$ and $F(v_n, t_n)$ maximised over all classes $y$. 

- Other $t$ from same class have higher dot products -> minimise loss 
- Other $t$ from different class have higher DP -> minimise loss  
- Other $t$ from same class have lower dot products -> do nothing since the difference term will be negative as the 0-1 loss $\Delta(y_n ,y)$ is 0 so the loss is 0. this makes sense because though if the average difference for $y_n$ negative but also the largest average difference across all classes, it follows that all other classes have a negative average difference which is moreoever larger and embeddings these classes must be therefore have less similar dot products on average. So actually is $y_n$ whose embeddings have the most similar dot products with $v$ compared to $t_n$. 
- Other $t$ from different class have lower DP -> minimise loss until the average difference is 1 (the negative of the 0-1 loss which is 1).

## 12. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks ([link](https://arxiv.org/abs/1703.03400v3))

For a set of tasks $\mathcal{T}_i$ you find the loss gradient and update separate parameters for each of the $K$ examples of the task using the 'train-train' data. The parameters of the model are now the 'adapted' parameters $\theta_i$. Then you find the average loss for each task using its adapted parameters and the 'train-test' data and find the gradients of these with respect to the shared parameters. So basically you do what is like a training step for each task, testing the resulting models using task specific parameters and finally learning shared parameters. To evaluate you fine-tune for 1 or more steps on 'test-train' examples and find the metrics on the 'test-test' examples. At the time it was published it beat state of the art on few-shot learning tasks for Omniglot and ImageNet.

## 13. Joint Object Detection and Multi-Object Tracking with Graph Neural Networks ([link](https://arxiv.org/pdf/2006.13164v2))
The model jointly learns to detect and track objects by linking features between frames using a Graph Neural Network. Feature maps are generated for pair of consecutive frames (time steps $t-1$ and $t$). CenterNet is used to detect objects in the frame at timestep $t$. 

To associate objects between frames, an identity embedding is generated for each object. RoIAlign is used to crop features for the detections that have already been made in the previous frame. Each pixel in frame $t$ is considered as a potential object's centre location and a graph is construct between the objects in the previous frame and all the pixels features within a neighbourhood in frame $t$. This is empirically justified since objects don't move too much across frames and avoids having to consider all possible pairs which would be computationally expensive.

Features in each layer of the graph network are updated using GraphConv as follows:
- Generate features for each node via a layer $\rho_1$
- Generate features for each of its neighbours via $\rho_2$
- Sum all of these to get the output feature:

    $$h_l^i = \rho_i\left(h_{l-1}^i\right) + \sum_{j \ in \mathcal{N}(i)}\rho_2\left(\h_{l-1}^j)$$

Although nodes in framee $t$ are not directly connected to each other if the graph network has more that one layer information can propagate between detections in the same frame through tracklets.

The model can thus be jointly trained to detect and to associate detections with "tracklets" from earlier frames. At inference time I think you start by detecting objects in the first frame and then using those as tracklets for the subsequent frame. Objects are paired with previous objects by calculating a similarities between the identity embeddings of the detections in the present and tracklets from previous frames. The Hungarian algorithm is then used to determine the best assignment of detections to tracklets. They also continute to retain tracklets for a few frames even if they are not matched to any object in the subsequent frame and similarly unmatched detections above a confidence level are kept. 

They improve on the SoTA on a number of metrics on the MOTChallenges.

## 14. Learning to Simulate Complex Physics with Graph Networks ([link](https://arxiv.org/abs/2002.09405))
In this paper they train a model to generate physics simulations. They input the states of several particles (e.g. those that comprise simulated fluid) at a given time, where a state includes position, velocities from earlier timesteps, static material properties and global system properties like forces and global material properties. A simulation consists of a trajectory of consecutive states. The goal is then to predict the acceleration at the next timestep from which the velocity and position can be estimated using semi-implicit Euler integration which updates velocity using the previouss velocity and the predicted acceleration and position using the newly estimated velocity and the previous position. In this way the predicted acceleration can directly influence the position estimate.

A graph network is used in which each particle is assigned to a node with edges added between nodes that correspond to particles with a radius $R$. They try two approaches, one where the absolute values of the particles are used in which case edges don't have any features, and another where the relative positions are used in which edge features are based on the distance and displacement between the particles. A processor then runs $M$ steps of learned message passing updating the graph each time and returning a final graph. A decoder is then used to obtain the dynamics information from each note of the final graph.

At training time random pairs of consecutive timesteps are sampled from the training trajectories and the future state (here just the position) is predicted. At inference time this step is repeated to predict an entire trajectory. During inference the model's own noisy predictions are used to generate subsequent predictions. Because of this they also add noise to input positions during training to make the model more robust to noisy inputs. 

They manage to generate realistic looking simulations for a variety of materials and environments.

# 15. FetchSGD- Communication-Efficient Federated Learning with Sketching ([link](http://proceedings.mlr.press/v119/rothchild20a/rothchild20a.pdf))

In this paper they propose a new method of computing gradient updates in federated learning. The gradients are computed locally on the client as usual but they are compressed using a data structure called Count-Sketch $\mathcal{S}$. It turns out that $\mathcal{S}$ is linear so that $\mathcal{S}(x + y) = \mathcal{S}(x) + \mathcal{S}(x)$ which means that the summed values after Count-Sketch has been applied to gradients is equivalent to applying it after summing all the gradients. Here they say that $\text{topk}\mathcal{S}(g)$ is roughly the same as $\text{topk}\mathcal(g)$. 

An issue with FedAvg the classic approach is that it requires download and upload of the entire model each time. Moreover unlike other compression schemes FetchSGD only involves one communication round with any client. It also does not need to store state locally nor does it need to update all the weights each time. 

Experiments on different tasks and datasets show that it performs better at higher compression levels compared to other algorithms.


