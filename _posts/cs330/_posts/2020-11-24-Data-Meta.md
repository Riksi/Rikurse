---
layout: post
title:  "First steps in Meta Learning"
date:   2020-11-25 12:01:20 +0000
categories: rc cs330
---

## Data

The key idea I needed to get my head around when first encountering meta learning was how data was handled.  The data is split as usual into train, val, test but the split is across classes rather than across examples. 

![A block of split across classes into train, val test]({{ site.baseurl }}/assets/split_across_classes.jpeg)

An input is a sequence of data points from a few classes.  Mini-distributions are created by sampling subsets of $N$ classes from a larger dataset. These mini training sets typically consist of just one or a very few examples per class hence one / few shot learning. The classes in the sample are arbitrarily assigned e.g. from $0 … N-1$ without regard to the classes in the original dataset. 

![The steps to sample $N=3$ classes and $K=2$ examples per class plus a test example from one of these classes to construct an input sequence]({{ site.baseurl }}/assets/n-way-k-shot.jpg)

Each input is not considered a sample from a distribution over data points but a sample from a distribution of training sets or “tasks”. A task does not need to be what we normally consider a task but it usually describes problems are in some way different from each other but also share some structure.

For example we might normally train models for them independently but use similar kinds of models. The mini training sets described above represent tasks that all involve the same problem (such as image classification in the SNAIL model discussed below) but what makes them "tasks" is that they are treated like separate datasets each of which may have kinds of data not present in others. 



A certain number of the elements of the sequence are the “training” examples of this mini training set and the rest are ”test” examples. 

![A train, val and test input sequence constructed in the manner described in the test]({{ site.baseurl }}/assets/input-examples.jpg)

The model is fed pairs of data points and labels from the training elements of the mini-distribution and learns to predict the class of the test elements of this mini-distribution. The model is evaluated based on how well it can learn a task after seeing just a few datapoint, label pairs.  At evaluation time, the model is fed inputs in a similar way and evaluated based on its prediction of the test elements of the sequence. 

## Model
I have been following the curriculum of [Stanford Deep Multi-Task and Meta Learning](https://cs330.stanford.edu/) and the first homework assignment involves rather very simple architecture so I decided to implement the [SNAIL](https://arxiv.org/abs/1707.03141) model instead, which is a [transformer](https://arxiv.org/abs/1706.03762)-style attention based sequence model.

<p align="center">
<img src="https://drive.google.com/uc?export=view&id=1z2BKCbREwE8y7ZDPpboNlt4jp-c87Ib9" height=500 style="margin-left">
</p>
<p align="center">
<img src="https://drive.google.com/uc?export=view&id=1LmSV157LWA6FmnW3eXzDgmi2BpDO76fe" width=800>
</p>
(From Figure 1 of [\[1\]](https://arxiv.org/abs/1707.03141))

<br>
I found it rather a painful process to implement SNAIL and even now I am not entirely satisfied with the results. The paper provides full details about the model architectures used including pseudo-code but remarkably little information, at least for the image classification experiments, about training such as the number of episodes, batch size, number of samples used for evaluation, etc.

CS330 homework 1 had some [instructions](https://cs330.stanford.edu/material/CS330_HW1.pdf) and [starter code](https://colab.research.google.com/drive/1slBqgKa20iTatoWThMWZTnFysgAVD1vh?usp=sharing) that helped me to some extent and I also came across [this](https://github.com/eambutu/snail-pytorch) PyTorch on Github which was helpful although I didn’t want to 'cheat' by referring to it too much. 

You can find a Colab notebook with my implementation [here](https://colab.research.google.com/drive/163jeJjY7ZHGjcgdjbr8m6UdrpjXH0AzX?usp=sharing) but use at your own risk. So far I have run experiments only for the 1-shot 5-way setting and although the results are reasonable (~98.7% test accuracy compared to the reported 99.07% $\pm$ 0.16%), I have not checked them carefully. 

## References

[1] [Stanford Deep Multi-Task and Meta Learning](https://cs330.stanford.edu/)

[2] [SNAIL](https://arxiv.org/abs/1707.03141)

